{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country', 'metal', 'jazz', 'hiphop', 'blues', 'classical', 'reggae', 'rock', 'pop', 'disco']\n"
     ]
    }
   ],
   "source": [
    "# GTZAN dataset\n",
    "general_path = 'Data'\n",
    "print(list(os.listdir(f'{general_path}/genres_original/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Steps\n",
    "# Ensure all MEL-spectrograms are same NxN dimensions\n",
    "# Convert them into compressed vector representations\n",
    "# create train-test-split,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>length</th>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc16_var</th>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <th>mfcc17_var</th>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <th>mfcc18_var</th>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <th>mfcc19_var</th>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <th>mfcc20_var</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blues.00000.0.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.335406</td>\n",
       "      <td>0.091048</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>1773.065032</td>\n",
       "      <td>167541.630869</td>\n",
       "      <td>1972.744388</td>\n",
       "      <td>117335.771563</td>\n",
       "      <td>...</td>\n",
       "      <td>39.687145</td>\n",
       "      <td>-3.241280</td>\n",
       "      <td>36.488243</td>\n",
       "      <td>0.722209</td>\n",
       "      <td>38.099152</td>\n",
       "      <td>-5.050335</td>\n",
       "      <td>33.618073</td>\n",
       "      <td>-0.243027</td>\n",
       "      <td>43.771767</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blues.00000.1.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.343065</td>\n",
       "      <td>0.086147</td>\n",
       "      <td>0.112699</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>1816.693777</td>\n",
       "      <td>90525.690866</td>\n",
       "      <td>2010.051501</td>\n",
       "      <td>65671.875673</td>\n",
       "      <td>...</td>\n",
       "      <td>64.748276</td>\n",
       "      <td>-6.055294</td>\n",
       "      <td>40.677654</td>\n",
       "      <td>0.159015</td>\n",
       "      <td>51.264091</td>\n",
       "      <td>-2.837699</td>\n",
       "      <td>97.030830</td>\n",
       "      <td>5.784063</td>\n",
       "      <td>59.943081</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blues.00000.2.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.346815</td>\n",
       "      <td>0.092243</td>\n",
       "      <td>0.132003</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>1788.539719</td>\n",
       "      <td>111407.437613</td>\n",
       "      <td>2084.565132</td>\n",
       "      <td>75124.921716</td>\n",
       "      <td>...</td>\n",
       "      <td>67.336563</td>\n",
       "      <td>-1.768610</td>\n",
       "      <td>28.348579</td>\n",
       "      <td>2.378768</td>\n",
       "      <td>45.717648</td>\n",
       "      <td>-1.938424</td>\n",
       "      <td>53.050835</td>\n",
       "      <td>2.517375</td>\n",
       "      <td>33.105122</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blues.00000.3.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.363639</td>\n",
       "      <td>0.086856</td>\n",
       "      <td>0.132565</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>1655.289045</td>\n",
       "      <td>111952.284517</td>\n",
       "      <td>1960.039988</td>\n",
       "      <td>82913.639269</td>\n",
       "      <td>...</td>\n",
       "      <td>47.739452</td>\n",
       "      <td>-3.841155</td>\n",
       "      <td>28.337118</td>\n",
       "      <td>1.218588</td>\n",
       "      <td>34.770935</td>\n",
       "      <td>-3.580352</td>\n",
       "      <td>50.836224</td>\n",
       "      <td>3.630866</td>\n",
       "      <td>32.023678</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blues.00000.4.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.335579</td>\n",
       "      <td>0.088129</td>\n",
       "      <td>0.143289</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>1630.656199</td>\n",
       "      <td>79667.267654</td>\n",
       "      <td>1948.503884</td>\n",
       "      <td>60204.020268</td>\n",
       "      <td>...</td>\n",
       "      <td>30.336359</td>\n",
       "      <td>0.664582</td>\n",
       "      <td>45.880913</td>\n",
       "      <td>1.689446</td>\n",
       "      <td>51.363583</td>\n",
       "      <td>-3.392489</td>\n",
       "      <td>26.738789</td>\n",
       "      <td>0.536961</td>\n",
       "      <td>29.146694</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  length  chroma_stft_mean  chroma_stft_var  rms_mean  \\\n",
       "0  blues.00000.0.wav   66149          0.335406         0.091048  0.130405   \n",
       "1  blues.00000.1.wav   66149          0.343065         0.086147  0.112699   \n",
       "2  blues.00000.2.wav   66149          0.346815         0.092243  0.132003   \n",
       "3  blues.00000.3.wav   66149          0.363639         0.086856  0.132565   \n",
       "4  blues.00000.4.wav   66149          0.335579         0.088129  0.143289   \n",
       "\n",
       "    rms_var  spectral_centroid_mean  spectral_centroid_var  \\\n",
       "0  0.003521             1773.065032          167541.630869   \n",
       "1  0.001450             1816.693777           90525.690866   \n",
       "2  0.004620             1788.539719          111407.437613   \n",
       "3  0.002448             1655.289045          111952.284517   \n",
       "4  0.001701             1630.656199           79667.267654   \n",
       "\n",
       "   spectral_bandwidth_mean  spectral_bandwidth_var  ...  mfcc16_var  \\\n",
       "0              1972.744388           117335.771563  ...   39.687145   \n",
       "1              2010.051501            65671.875673  ...   64.748276   \n",
       "2              2084.565132            75124.921716  ...   67.336563   \n",
       "3              1960.039988            82913.639269  ...   47.739452   \n",
       "4              1948.503884            60204.020268  ...   30.336359   \n",
       "\n",
       "   mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  mfcc19_var  \\\n",
       "0    -3.241280   36.488243     0.722209   38.099152    -5.050335   33.618073   \n",
       "1    -6.055294   40.677654     0.159015   51.264091    -2.837699   97.030830   \n",
       "2    -1.768610   28.348579     2.378768   45.717648    -1.938424   53.050835   \n",
       "3    -3.841155   28.337118     1.218588   34.770935    -3.580352   50.836224   \n",
       "4     0.664582   45.880913     1.689446   51.363583    -3.392489   26.738789   \n",
       "\n",
       "   mfcc20_mean  mfcc20_var  label  \n",
       "0    -0.243027   43.771767  blues  \n",
       "1     5.784063   59.943081  blues  \n",
       "2     2.517375   33.105122  blues  \n",
       "3     3.630866   32.023678  blues  \n",
       "4     0.536961   29.146694  blues  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/features_3_sec.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9990, 60)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <th>rolloff_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc16_var</th>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <th>mfcc17_var</th>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <th>mfcc18_var</th>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <th>mfcc19_var</th>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <th>mfcc20_var</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66149</td>\n",
       "      <td>0.335406</td>\n",
       "      <td>0.091048</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>1773.065032</td>\n",
       "      <td>167541.630869</td>\n",
       "      <td>1972.744388</td>\n",
       "      <td>117335.771563</td>\n",
       "      <td>3714.560359</td>\n",
       "      <td>...</td>\n",
       "      <td>39.687145</td>\n",
       "      <td>-3.241280</td>\n",
       "      <td>36.488243</td>\n",
       "      <td>0.722209</td>\n",
       "      <td>38.099152</td>\n",
       "      <td>-5.050335</td>\n",
       "      <td>33.618073</td>\n",
       "      <td>-0.243027</td>\n",
       "      <td>43.771767</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66149</td>\n",
       "      <td>0.343065</td>\n",
       "      <td>0.086147</td>\n",
       "      <td>0.112699</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>1816.693777</td>\n",
       "      <td>90525.690866</td>\n",
       "      <td>2010.051501</td>\n",
       "      <td>65671.875673</td>\n",
       "      <td>3869.682242</td>\n",
       "      <td>...</td>\n",
       "      <td>64.748276</td>\n",
       "      <td>-6.055294</td>\n",
       "      <td>40.677654</td>\n",
       "      <td>0.159015</td>\n",
       "      <td>51.264091</td>\n",
       "      <td>-2.837699</td>\n",
       "      <td>97.030830</td>\n",
       "      <td>5.784063</td>\n",
       "      <td>59.943081</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66149</td>\n",
       "      <td>0.346815</td>\n",
       "      <td>0.092243</td>\n",
       "      <td>0.132003</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>1788.539719</td>\n",
       "      <td>111407.437613</td>\n",
       "      <td>2084.565132</td>\n",
       "      <td>75124.921716</td>\n",
       "      <td>3997.639160</td>\n",
       "      <td>...</td>\n",
       "      <td>67.336563</td>\n",
       "      <td>-1.768610</td>\n",
       "      <td>28.348579</td>\n",
       "      <td>2.378768</td>\n",
       "      <td>45.717648</td>\n",
       "      <td>-1.938424</td>\n",
       "      <td>53.050835</td>\n",
       "      <td>2.517375</td>\n",
       "      <td>33.105122</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66149</td>\n",
       "      <td>0.363639</td>\n",
       "      <td>0.086856</td>\n",
       "      <td>0.132565</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>1655.289045</td>\n",
       "      <td>111952.284517</td>\n",
       "      <td>1960.039988</td>\n",
       "      <td>82913.639269</td>\n",
       "      <td>3568.300218</td>\n",
       "      <td>...</td>\n",
       "      <td>47.739452</td>\n",
       "      <td>-3.841155</td>\n",
       "      <td>28.337118</td>\n",
       "      <td>1.218588</td>\n",
       "      <td>34.770935</td>\n",
       "      <td>-3.580352</td>\n",
       "      <td>50.836224</td>\n",
       "      <td>3.630866</td>\n",
       "      <td>32.023678</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66149</td>\n",
       "      <td>0.335579</td>\n",
       "      <td>0.088129</td>\n",
       "      <td>0.143289</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>1630.656199</td>\n",
       "      <td>79667.267654</td>\n",
       "      <td>1948.503884</td>\n",
       "      <td>60204.020268</td>\n",
       "      <td>3469.992864</td>\n",
       "      <td>...</td>\n",
       "      <td>30.336359</td>\n",
       "      <td>0.664582</td>\n",
       "      <td>45.880913</td>\n",
       "      <td>1.689446</td>\n",
       "      <td>51.363583</td>\n",
       "      <td>-3.392489</td>\n",
       "      <td>26.738789</td>\n",
       "      <td>0.536961</td>\n",
       "      <td>29.146694</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  chroma_stft_mean  chroma_stft_var  rms_mean   rms_var  \\\n",
       "0   66149          0.335406         0.091048  0.130405  0.003521   \n",
       "1   66149          0.343065         0.086147  0.112699  0.001450   \n",
       "2   66149          0.346815         0.092243  0.132003  0.004620   \n",
       "3   66149          0.363639         0.086856  0.132565  0.002448   \n",
       "4   66149          0.335579         0.088129  0.143289  0.001701   \n",
       "\n",
       "   spectral_centroid_mean  spectral_centroid_var  spectral_bandwidth_mean  \\\n",
       "0             1773.065032          167541.630869              1972.744388   \n",
       "1             1816.693777           90525.690866              2010.051501   \n",
       "2             1788.539719          111407.437613              2084.565132   \n",
       "3             1655.289045          111952.284517              1960.039988   \n",
       "4             1630.656199           79667.267654              1948.503884   \n",
       "\n",
       "   spectral_bandwidth_var  rolloff_mean  ...  mfcc16_var  mfcc17_mean  \\\n",
       "0           117335.771563   3714.560359  ...   39.687145    -3.241280   \n",
       "1            65671.875673   3869.682242  ...   64.748276    -6.055294   \n",
       "2            75124.921716   3997.639160  ...   67.336563    -1.768610   \n",
       "3            82913.639269   3568.300218  ...   47.739452    -3.841155   \n",
       "4            60204.020268   3469.992864  ...   30.336359     0.664582   \n",
       "\n",
       "   mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  mfcc19_var  mfcc20_mean  \\\n",
       "0   36.488243     0.722209   38.099152    -5.050335   33.618073    -0.243027   \n",
       "1   40.677654     0.159015   51.264091    -2.837699   97.030830     5.784063   \n",
       "2   28.348579     2.378768   45.717648    -1.938424   53.050835     2.517375   \n",
       "3   28.337118     1.218588   34.770935    -3.580352   50.836224     3.630866   \n",
       "4   45.880913     1.689446   51.363583    -3.392489   26.738789     0.536961   \n",
       "\n",
       "   mfcc20_var  label  \n",
       "0   43.771767  blues  \n",
       "1   59.943081  blues  \n",
       "2   33.105122  blues  \n",
       "3   32.023678  blues  \n",
       "4   29.146694  blues  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Some logic derived from reading through the following Kaggle set: https://www.kaggle.com/code/aishwarya2210/let-s-tune-the-music-with-cnn-xgboost/notebook\n",
    "This is for scaling and extracting the features as needed, and converting the labels to numerical values. \n",
    "This will help to create the training and testing sets, then will be fed into the model\n",
    "'''\n",
    "\n",
    "# Label Encoding - encod the categorical classes with numerical integer values for training\n",
    "\n",
    "# Blues - 0\n",
    "# Classical - 1\n",
    "# Country - 2\n",
    "# Disco - 3\n",
    "# Hip-hop - 4 \n",
    "# Jazz - 5  \n",
    "# Metal - 6 \n",
    "# Pop - 7\n",
    "# Reggae - 8\n",
    "# Rock - 9\n",
    "\n",
    "#To convert categorical data into model-understandable numerica data\n",
    "class_list = df.iloc[:, -1]\n",
    "convertor = LabelEncoder()\n",
    "\n",
    "df = df.drop(labels=\"filename\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the label encoder & return encoded labels\n",
    "y = convertor.fit_transform(class_list)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fit = StandardScaler()\n",
    "X = fit.fit_transform(np.array(df.iloc[:, :-1], dtype = float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6993, 58)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(df.iloc[:,:-1],y,test_size=0.3)\n",
    "x_train.head()\n",
    "# y_train.item(1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[4995, 57], edge_index=[2, 2494503], y=[4995], train_mask=[4995], val_mask=[4995], test_mask=[4995])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REDUCE DATASET TO A TENTH OF ITS SIZE\n",
    "sample_size = df.shape[0] // 2  # Reduce to 10% of the original size\n",
    "sampled_indices = df.sample(n=sample_size, random_state=42).index  # Randomly sample indices\n",
    "df = df.loc[sampled_indices]  # Apply sampling to df\n",
    "y = y[sampled_indices]  # Apply the same sampling to y\n",
    "\n",
    "\n",
    "# Removing other unecessary variables\n",
    "df = df.drop(labels=\"length\", axis=1)\n",
    "df = df.drop(labels=\"label\", axis=1)\n",
    "\n",
    "# Creataing edge tensor, 1 - Identity Matrix --> tensor\n",
    "adj_matrix = 1 - torch.eye(sample_size)\n",
    "edge_index = torch.nonzero(adj_matrix, as_tuple=False).t()\n",
    "\n",
    "\n",
    "\n",
    "# Randomly drop 90% of the edges\n",
    "num_edges = edge_index.shape[1]  # Total number of edges\n",
    "num_edges_to_keep = num_edges // 10  # Keep only half the edges\n",
    "\n",
    "# Shuffle the edges randomly and keep only half\n",
    "perm = torch.randperm(num_edges)  # Generate a random permutation of indices\n",
    "edge_index = edge_index[:, perm[:num_edges_to_keep]]  # Select only half of the edges\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setting data features to tensor\n",
    "x = torch.tensor(df.values)\n",
    "x = x.to(torch.float32)\n",
    "\n",
    "# Normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x = torch.tensor(scaler.fit_transform(df.values), dtype=torch.float32)\n",
    "\n",
    "# Setting y to tensor\n",
    "y = torch.tensor(y, dtype=torch.long)  # Make sure target is of type long (integer labels)\n",
    "\n",
    "# Create data object\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Setup test/train/validation split 80/10/10:\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split the data into 80% training and 20% remaining (test + validation)\n",
    "train_indices, remaining_indices = train_test_split(\n",
    "    range(sample_size), test_size=0.2, stratify=y.numpy(), random_state=42\n",
    ")\n",
    "# split the remaining 20% into 50% for validation and 50% for testing\n",
    "val_indices, test_indices = train_test_split(\n",
    "    remaining_indices, test_size=0.5, stratify=y.numpy()[remaining_indices], random_state=42\n",
    ")\n",
    "# initialize masks for each split\n",
    "train_mask = torch.zeros(sample_size, dtype=torch.bool)\n",
    "val_mask = torch.zeros(sample_size, dtype=torch.bool)\n",
    "test_mask = torch.zeros(sample_size, dtype=torch.bool)\n",
    "# set the corresponding entries to True for each mask\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "# assign the masks to the data object\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Check dimmensions\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([4995, 57])\n",
      "edge_index shape: torch.Size([2, 2494503])\n",
      "y shape: torch.Size([4995])\n",
      "train_mask shape: torch.Size([4995])\n",
      "val_mask shape: torch.Size([4995])\n",
      "test_mask shape: torch.Size([4995])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x shape: {data.x.shape}\")\n",
    "print(f\"edge_index shape: {data.edge_index.shape}\")\n",
    "print(f\"y shape: {data.y.shape}\")\n",
    "print(f\"train_mask shape: {data.train_mask.shape}\")\n",
    "print(f\"val_mask shape: {data.val_mask.shape}\")\n",
    "print(f\"test_mask shape: {data.test_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (layer1): GCNConv(57, 57)\n",
      "  (bn1): BatchNorm1d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (layer2): GCNConv(57, 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Modified from https://www.geeksforgeeks.org/graph-neural-networks-with-pytorch/\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layer1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim)  # Optional, for normalization\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.layer2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First layer\n",
    "        h = self.layer1(x, edge_index)\n",
    "        h = self.bn1(h)  # Optional batch normalization\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        # Second layer\n",
    "        z = self.layer2(h, edge_index)\n",
    "        \n",
    "        return z\n",
    "\n",
    "model_test = GCN(df.shape[1], df.shape[1], 10, 0.5)\n",
    "print(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GNN model\n",
    "input_feature_count = df.shape[1]\n",
    "hidden_dim = df.shape[1]  # PICK!\n",
    "num_classes = 10\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "dropout_rate = 0.5\n",
    "num_epochs = 1000\n",
    "\n",
    "model = GCN(input_feature_count, hidden_dim, num_classes, dropout_rate)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "graph_data = data  # Get the graph data\n",
    "\n",
    "# Cross-entropy loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Pre-allocate arrays for loss, training accuracy, and validation accuracy\n",
    "losses = torch.zeros(num_epochs)\n",
    "train_accs = torch.zeros(num_epochs)\n",
    "val_accs = torch.zeros(num_epochs)\n",
    "\n",
    "def train_model():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(graph_data.x, graph_data.edge_index)\n",
    "    loss = criterion(output[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Training accuracy\n",
    "    predictions = output.argmax(dim=1)\n",
    "    correct = (predictions[graph_data.train_mask] == graph_data.y[graph_data.train_mask]).sum()\n",
    "    train_acc = int(correct) / int(graph_data.train_mask.sum())\n",
    "    return loss.item(), train_acc\n",
    "\n",
    "def evaluate_model(mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(graph_data.x, graph_data.edge_index)\n",
    "        predictions = output.argmax(dim=1)\n",
    "        correct = (predictions[mask] == graph_data.y[mask]).sum()\n",
    "        acc = int(correct) / int(mask.sum())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 2.3961, Train Acc: 0.1011, Val Acc: 0.1002\n",
      "Epoch: 002, Loss: 2.3280, Train Acc: 0.0991, Val Acc: 0.1062\n",
      "Epoch: 003, Loss: 2.3073, Train Acc: 0.1046, Val Acc: 0.1042\n",
      "Epoch: 004, Loss: 2.3089, Train Acc: 0.1076, Val Acc: 0.1002\n",
      "Epoch: 005, Loss: 2.3194, Train Acc: 0.1101, Val Acc: 0.1002\n",
      "Epoch: 006, Loss: 2.3234, Train Acc: 0.1061, Val Acc: 0.1042\n",
      "Epoch: 007, Loss: 2.3227, Train Acc: 0.1056, Val Acc: 0.1042\n",
      "Epoch: 008, Loss: 2.3188, Train Acc: 0.1089, Val Acc: 0.1042\n",
      "Epoch: 009, Loss: 2.3105, Train Acc: 0.1144, Val Acc: 0.1042\n",
      "Epoch: 010, Loss: 2.3014, Train Acc: 0.1234, Val Acc: 0.0862\n",
      "Epoch: 011, Loss: 2.2981, Train Acc: 0.1189, Val Acc: 0.0942\n",
      "Epoch: 012, Loss: 2.2983, Train Acc: 0.1109, Val Acc: 0.0902\n",
      "Epoch: 013, Loss: 2.2994, Train Acc: 0.1071, Val Acc: 0.0942\n",
      "Epoch: 014, Loss: 2.3010, Train Acc: 0.1084, Val Acc: 0.1022\n",
      "Epoch: 015, Loss: 2.3029, Train Acc: 0.1139, Val Acc: 0.1042\n",
      "Epoch: 016, Loss: 2.3034, Train Acc: 0.1134, Val Acc: 0.1022\n",
      "Epoch: 017, Loss: 2.3000, Train Acc: 0.1131, Val Acc: 0.1042\n",
      "Epoch: 018, Loss: 2.2980, Train Acc: 0.1161, Val Acc: 0.1042\n",
      "Epoch: 019, Loss: 2.2951, Train Acc: 0.1219, Val Acc: 0.1042\n",
      "Epoch: 020, Loss: 2.2919, Train Acc: 0.1319, Val Acc: 0.1042\n",
      "Epoch: 021, Loss: 2.2914, Train Acc: 0.1316, Val Acc: 0.1002\n",
      "Epoch: 022, Loss: 2.2914, Train Acc: 0.1336, Val Acc: 0.1002\n",
      "Epoch: 023, Loss: 2.2935, Train Acc: 0.1256, Val Acc: 0.1062\n",
      "Epoch: 024, Loss: 2.2930, Train Acc: 0.1319, Val Acc: 0.1062\n",
      "Epoch: 025, Loss: 2.2927, Train Acc: 0.1291, Val Acc: 0.0982\n",
      "Epoch: 026, Loss: 2.2915, Train Acc: 0.1301, Val Acc: 0.1022\n",
      "Epoch: 027, Loss: 2.2911, Train Acc: 0.1346, Val Acc: 0.1022\n",
      "Epoch: 028, Loss: 2.2884, Train Acc: 0.1391, Val Acc: 0.1022\n",
      "Epoch: 029, Loss: 2.2874, Train Acc: 0.1439, Val Acc: 0.1002\n",
      "Epoch: 030, Loss: 2.2881, Train Acc: 0.1441, Val Acc: 0.1022\n",
      "Epoch: 031, Loss: 2.2882, Train Acc: 0.1266, Val Acc: 0.1022\n",
      "Epoch: 032, Loss: 2.2870, Train Acc: 0.1291, Val Acc: 0.1102\n",
      "Epoch: 033, Loss: 2.2877, Train Acc: 0.1354, Val Acc: 0.1102\n",
      "Epoch: 034, Loss: 2.2868, Train Acc: 0.1346, Val Acc: 0.1062\n",
      "Epoch: 035, Loss: 2.2874, Train Acc: 0.1444, Val Acc: 0.1062\n",
      "Epoch: 036, Loss: 2.2853, Train Acc: 0.1537, Val Acc: 0.1042\n",
      "Epoch: 037, Loss: 2.2852, Train Acc: 0.1486, Val Acc: 0.1042\n",
      "Epoch: 038, Loss: 2.2838, Train Acc: 0.1592, Val Acc: 0.1042\n",
      "Epoch: 039, Loss: 2.2834, Train Acc: 0.1514, Val Acc: 0.1042\n",
      "Epoch: 040, Loss: 2.2834, Train Acc: 0.1512, Val Acc: 0.1042\n",
      "Epoch: 041, Loss: 2.2843, Train Acc: 0.1517, Val Acc: 0.1022\n",
      "Epoch: 042, Loss: 2.2831, Train Acc: 0.1547, Val Acc: 0.1002\n",
      "Epoch: 043, Loss: 2.2840, Train Acc: 0.1569, Val Acc: 0.1002\n",
      "Epoch: 044, Loss: 2.2841, Train Acc: 0.1409, Val Acc: 0.1002\n",
      "Epoch: 045, Loss: 2.2824, Train Acc: 0.1489, Val Acc: 0.1022\n",
      "Epoch: 046, Loss: 2.2818, Train Acc: 0.1569, Val Acc: 0.1102\n",
      "Epoch: 047, Loss: 2.2823, Train Acc: 0.1604, Val Acc: 0.1142\n",
      "Epoch: 048, Loss: 2.2819, Train Acc: 0.1582, Val Acc: 0.1122\n",
      "Epoch: 049, Loss: 2.2814, Train Acc: 0.1584, Val Acc: 0.1042\n",
      "Epoch: 050, Loss: 2.2807, Train Acc: 0.1569, Val Acc: 0.1102\n",
      "Epoch: 051, Loss: 2.2818, Train Acc: 0.1714, Val Acc: 0.1062\n",
      "Epoch: 052, Loss: 2.2807, Train Acc: 0.1637, Val Acc: 0.1062\n",
      "Epoch: 053, Loss: 2.2812, Train Acc: 0.1659, Val Acc: 0.1022\n",
      "Epoch: 054, Loss: 2.2804, Train Acc: 0.1624, Val Acc: 0.1062\n",
      "Epoch: 055, Loss: 2.2804, Train Acc: 0.1614, Val Acc: 0.1022\n",
      "Epoch: 056, Loss: 2.2811, Train Acc: 0.1624, Val Acc: 0.1102\n",
      "Epoch: 057, Loss: 2.2807, Train Acc: 0.1654, Val Acc: 0.1082\n",
      "Epoch: 058, Loss: 2.2799, Train Acc: 0.1642, Val Acc: 0.1082\n",
      "Epoch: 059, Loss: 2.2793, Train Acc: 0.1532, Val Acc: 0.1142\n",
      "Epoch: 060, Loss: 2.2800, Train Acc: 0.1584, Val Acc: 0.1162\n",
      "Epoch: 061, Loss: 2.2783, Train Acc: 0.1592, Val Acc: 0.1182\n",
      "Epoch: 062, Loss: 2.2792, Train Acc: 0.1654, Val Acc: 0.1242\n",
      "Epoch: 063, Loss: 2.2776, Train Acc: 0.1714, Val Acc: 0.1283\n",
      "Epoch: 064, Loss: 2.2784, Train Acc: 0.1749, Val Acc: 0.1283\n",
      "Epoch: 065, Loss: 2.2782, Train Acc: 0.1722, Val Acc: 0.1263\n",
      "Epoch: 066, Loss: 2.2792, Train Acc: 0.1654, Val Acc: 0.1263\n",
      "Epoch: 067, Loss: 2.2779, Train Acc: 0.1664, Val Acc: 0.1242\n",
      "Epoch: 068, Loss: 2.2793, Train Acc: 0.1649, Val Acc: 0.1222\n",
      "Epoch: 069, Loss: 2.2778, Train Acc: 0.1622, Val Acc: 0.1283\n",
      "Epoch: 070, Loss: 2.2785, Train Acc: 0.1629, Val Acc: 0.1162\n",
      "Epoch: 071, Loss: 2.2778, Train Acc: 0.1789, Val Acc: 0.1062\n",
      "Epoch: 072, Loss: 2.2775, Train Acc: 0.1734, Val Acc: 0.1162\n",
      "Epoch: 073, Loss: 2.2773, Train Acc: 0.1687, Val Acc: 0.1323\n",
      "Epoch: 074, Loss: 2.2778, Train Acc: 0.1632, Val Acc: 0.1303\n",
      "Epoch: 075, Loss: 2.2772, Train Acc: 0.1572, Val Acc: 0.1363\n",
      "Epoch: 076, Loss: 2.2779, Train Acc: 0.1604, Val Acc: 0.1383\n",
      "Epoch: 077, Loss: 2.2771, Train Acc: 0.1622, Val Acc: 0.1303\n",
      "Epoch: 078, Loss: 2.2766, Train Acc: 0.1829, Val Acc: 0.1202\n",
      "Epoch: 079, Loss: 2.2769, Train Acc: 0.1757, Val Acc: 0.1222\n",
      "Epoch: 080, Loss: 2.2765, Train Acc: 0.1822, Val Acc: 0.1283\n",
      "Epoch: 081, Loss: 2.2768, Train Acc: 0.1639, Val Acc: 0.1242\n",
      "Epoch: 082, Loss: 2.2771, Train Acc: 0.1617, Val Acc: 0.1263\n",
      "Epoch: 083, Loss: 2.2757, Train Acc: 0.1639, Val Acc: 0.1303\n",
      "Epoch: 084, Loss: 2.2763, Train Acc: 0.1652, Val Acc: 0.1383\n",
      "Epoch: 085, Loss: 2.2758, Train Acc: 0.1589, Val Acc: 0.1323\n",
      "Epoch: 086, Loss: 2.2763, Train Acc: 0.1657, Val Acc: 0.1263\n",
      "Epoch: 087, Loss: 2.2755, Train Acc: 0.1824, Val Acc: 0.1242\n",
      "Epoch: 088, Loss: 2.2763, Train Acc: 0.1769, Val Acc: 0.1182\n",
      "Epoch: 089, Loss: 2.2759, Train Acc: 0.1779, Val Acc: 0.1202\n",
      "Epoch: 090, Loss: 2.2761, Train Acc: 0.1729, Val Acc: 0.1182\n",
      "Epoch: 091, Loss: 2.2749, Train Acc: 0.1682, Val Acc: 0.1323\n",
      "Epoch: 092, Loss: 2.2762, Train Acc: 0.1732, Val Acc: 0.1263\n",
      "Epoch: 093, Loss: 2.2751, Train Acc: 0.1694, Val Acc: 0.1303\n",
      "Epoch: 094, Loss: 2.2751, Train Acc: 0.1697, Val Acc: 0.1222\n",
      "Epoch: 095, Loss: 2.2759, Train Acc: 0.1697, Val Acc: 0.1202\n",
      "Epoch: 096, Loss: 2.2759, Train Acc: 0.1699, Val Acc: 0.1202\n",
      "Epoch: 097, Loss: 2.2749, Train Acc: 0.1767, Val Acc: 0.1222\n",
      "Epoch: 098, Loss: 2.2757, Train Acc: 0.1724, Val Acc: 0.1283\n",
      "Epoch: 099, Loss: 2.2750, Train Acc: 0.1624, Val Acc: 0.1162\n",
      "Epoch: 100, Loss: 2.2748, Train Acc: 0.1617, Val Acc: 0.1283\n",
      "Epoch: 101, Loss: 2.2742, Train Acc: 0.1657, Val Acc: 0.1202\n",
      "Epoch: 102, Loss: 2.2735, Train Acc: 0.1727, Val Acc: 0.1162\n",
      "Epoch: 103, Loss: 2.2747, Train Acc: 0.1719, Val Acc: 0.1182\n",
      "Epoch: 104, Loss: 2.2736, Train Acc: 0.1769, Val Acc: 0.1102\n",
      "Epoch: 105, Loss: 2.2747, Train Acc: 0.1692, Val Acc: 0.1202\n",
      "Epoch: 106, Loss: 2.2743, Train Acc: 0.1707, Val Acc: 0.1142\n",
      "Epoch: 107, Loss: 2.2739, Train Acc: 0.1579, Val Acc: 0.1202\n",
      "Epoch: 108, Loss: 2.2723, Train Acc: 0.1774, Val Acc: 0.1343\n",
      "Epoch: 109, Loss: 2.2734, Train Acc: 0.1684, Val Acc: 0.1283\n",
      "Epoch: 110, Loss: 2.2730, Train Acc: 0.1789, Val Acc: 0.1162\n",
      "Epoch: 111, Loss: 2.2726, Train Acc: 0.1694, Val Acc: 0.1062\n",
      "Epoch: 112, Loss: 2.2716, Train Acc: 0.1767, Val Acc: 0.1182\n",
      "Epoch: 113, Loss: 2.2734, Train Acc: 0.1662, Val Acc: 0.1162\n",
      "Epoch: 114, Loss: 2.2730, Train Acc: 0.1767, Val Acc: 0.1303\n",
      "Epoch: 115, Loss: 2.2720, Train Acc: 0.1767, Val Acc: 0.1283\n",
      "Epoch: 116, Loss: 2.2716, Train Acc: 0.1722, Val Acc: 0.1303\n",
      "Epoch: 117, Loss: 2.2727, Train Acc: 0.1642, Val Acc: 0.1222\n",
      "Epoch: 118, Loss: 2.2725, Train Acc: 0.1637, Val Acc: 0.1323\n",
      "Epoch: 119, Loss: 2.2716, Train Acc: 0.1767, Val Acc: 0.1182\n",
      "Epoch: 120, Loss: 2.2717, Train Acc: 0.1749, Val Acc: 0.1182\n",
      "Epoch: 121, Loss: 2.2720, Train Acc: 0.1629, Val Acc: 0.1202\n",
      "Epoch: 122, Loss: 2.2721, Train Acc: 0.1722, Val Acc: 0.1062\n",
      "Epoch: 123, Loss: 2.2711, Train Acc: 0.1704, Val Acc: 0.1162\n",
      "Epoch: 124, Loss: 2.2715, Train Acc: 0.1609, Val Acc: 0.1303\n",
      "Epoch: 125, Loss: 2.2717, Train Acc: 0.1802, Val Acc: 0.1182\n",
      "Epoch: 126, Loss: 2.2701, Train Acc: 0.1782, Val Acc: 0.1162\n",
      "Epoch: 127, Loss: 2.2706, Train Acc: 0.1829, Val Acc: 0.1162\n",
      "Epoch: 128, Loss: 2.2718, Train Acc: 0.1854, Val Acc: 0.1142\n",
      "Epoch: 129, Loss: 2.2704, Train Acc: 0.1827, Val Acc: 0.1263\n",
      "Epoch: 130, Loss: 2.2709, Train Acc: 0.1597, Val Acc: 0.1263\n",
      "Epoch: 131, Loss: 2.2706, Train Acc: 0.1664, Val Acc: 0.1303\n",
      "Epoch: 132, Loss: 2.2703, Train Acc: 0.1774, Val Acc: 0.1303\n",
      "Epoch: 133, Loss: 2.2698, Train Acc: 0.1772, Val Acc: 0.1303\n",
      "Epoch: 134, Loss: 2.2698, Train Acc: 0.1744, Val Acc: 0.1323\n",
      "Epoch: 135, Loss: 2.2696, Train Acc: 0.1652, Val Acc: 0.1283\n",
      "Epoch: 136, Loss: 2.2704, Train Acc: 0.1669, Val Acc: 0.1303\n",
      "Epoch: 137, Loss: 2.2677, Train Acc: 0.1749, Val Acc: 0.1383\n",
      "Epoch: 138, Loss: 2.2705, Train Acc: 0.1824, Val Acc: 0.1363\n",
      "Epoch: 139, Loss: 2.2692, Train Acc: 0.1862, Val Acc: 0.1303\n",
      "Epoch: 140, Loss: 2.2689, Train Acc: 0.1667, Val Acc: 0.1343\n",
      "Epoch: 141, Loss: 2.2693, Train Acc: 0.1769, Val Acc: 0.1283\n",
      "Epoch: 142, Loss: 2.2680, Train Acc: 0.1807, Val Acc: 0.1283\n",
      "Epoch: 143, Loss: 2.2702, Train Acc: 0.1694, Val Acc: 0.1363\n",
      "Epoch: 144, Loss: 2.2685, Train Acc: 0.1754, Val Acc: 0.1363\n",
      "Epoch: 145, Loss: 2.2687, Train Acc: 0.1769, Val Acc: 0.1283\n",
      "Epoch: 146, Loss: 2.2691, Train Acc: 0.1697, Val Acc: 0.1303\n",
      "Epoch: 147, Loss: 2.2676, Train Acc: 0.1907, Val Acc: 0.1303\n",
      "Epoch: 148, Loss: 2.2685, Train Acc: 0.1784, Val Acc: 0.1182\n",
      "Epoch: 149, Loss: 2.2681, Train Acc: 0.1739, Val Acc: 0.1283\n",
      "Epoch: 150, Loss: 2.2666, Train Acc: 0.1747, Val Acc: 0.1162\n",
      "Epoch: 151, Loss: 2.2664, Train Acc: 0.1669, Val Acc: 0.1142\n",
      "Epoch: 152, Loss: 2.2690, Train Acc: 0.1539, Val Acc: 0.1222\n",
      "Epoch: 153, Loss: 2.2678, Train Acc: 0.1799, Val Acc: 0.1142\n",
      "Epoch: 154, Loss: 2.2686, Train Acc: 0.1794, Val Acc: 0.1242\n",
      "Epoch: 155, Loss: 2.2687, Train Acc: 0.1652, Val Acc: 0.1222\n",
      "Epoch: 156, Loss: 2.2672, Train Acc: 0.1714, Val Acc: 0.1323\n",
      "Epoch: 157, Loss: 2.2663, Train Acc: 0.1779, Val Acc: 0.1303\n",
      "Epoch: 158, Loss: 2.2680, Train Acc: 0.1679, Val Acc: 0.1283\n",
      "Epoch: 159, Loss: 2.2688, Train Acc: 0.1652, Val Acc: 0.1142\n",
      "Epoch: 160, Loss: 2.2673, Train Acc: 0.1732, Val Acc: 0.1122\n",
      "Epoch: 161, Loss: 2.2673, Train Acc: 0.1774, Val Acc: 0.1283\n",
      "Epoch: 162, Loss: 2.2659, Train Acc: 0.1714, Val Acc: 0.1463\n",
      "Epoch: 163, Loss: 2.2671, Train Acc: 0.1672, Val Acc: 0.1283\n",
      "Epoch: 164, Loss: 2.2680, Train Acc: 0.1644, Val Acc: 0.1283\n",
      "Epoch: 165, Loss: 2.2662, Train Acc: 0.1774, Val Acc: 0.1202\n",
      "Epoch: 166, Loss: 2.2663, Train Acc: 0.1769, Val Acc: 0.1162\n",
      "Epoch: 167, Loss: 2.2682, Train Acc: 0.1712, Val Acc: 0.1182\n",
      "Epoch: 168, Loss: 2.2653, Train Acc: 0.1754, Val Acc: 0.1222\n",
      "Epoch: 169, Loss: 2.2660, Train Acc: 0.1657, Val Acc: 0.1283\n",
      "Epoch: 170, Loss: 2.2658, Train Acc: 0.1587, Val Acc: 0.1323\n",
      "Epoch: 171, Loss: 2.2645, Train Acc: 0.1802, Val Acc: 0.1283\n",
      "Epoch: 172, Loss: 2.2654, Train Acc: 0.1847, Val Acc: 0.1222\n",
      "Epoch: 173, Loss: 2.2666, Train Acc: 0.1789, Val Acc: 0.1283\n",
      "Epoch: 174, Loss: 2.2651, Train Acc: 0.1754, Val Acc: 0.1363\n",
      "Epoch: 175, Loss: 2.2648, Train Acc: 0.1634, Val Acc: 0.1383\n",
      "Epoch: 176, Loss: 2.2645, Train Acc: 0.1689, Val Acc: 0.1162\n",
      "Epoch: 177, Loss: 2.2648, Train Acc: 0.1742, Val Acc: 0.1102\n",
      "Epoch: 178, Loss: 2.2673, Train Acc: 0.1692, Val Acc: 0.1283\n",
      "Epoch: 179, Loss: 2.2653, Train Acc: 0.1784, Val Acc: 0.1062\n",
      "Epoch: 180, Loss: 2.2657, Train Acc: 0.1792, Val Acc: 0.1162\n",
      "Epoch: 181, Loss: 2.2661, Train Acc: 0.1739, Val Acc: 0.1082\n",
      "Epoch: 182, Loss: 2.2650, Train Acc: 0.1597, Val Acc: 0.1142\n",
      "Epoch: 183, Loss: 2.2649, Train Acc: 0.1624, Val Acc: 0.1222\n",
      "Epoch: 184, Loss: 2.2623, Train Acc: 0.1767, Val Acc: 0.1162\n",
      "Epoch: 185, Loss: 2.2623, Train Acc: 0.1907, Val Acc: 0.1062\n",
      "Epoch: 186, Loss: 2.2637, Train Acc: 0.1782, Val Acc: 0.1142\n",
      "Epoch: 187, Loss: 2.2626, Train Acc: 0.1789, Val Acc: 0.1182\n",
      "Epoch: 188, Loss: 2.2626, Train Acc: 0.1737, Val Acc: 0.1283\n",
      "Epoch: 189, Loss: 2.2637, Train Acc: 0.1594, Val Acc: 0.1102\n",
      "Epoch: 190, Loss: 2.2627, Train Acc: 0.1709, Val Acc: 0.0982\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      3\u001b[0m     loss_value, train_acc \u001b[38;5;241m=\u001b[39m train_model()\n\u001b[0;32m----> 4\u001b[0m     val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Save loss and accuracy for each epoch in pre-allocated arrays\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     losses[epoch] \u001b[38;5;241m=\u001b[39m loss_value\n",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(mask)\u001b[0m\n\u001b[1;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m     correct \u001b[38;5;241m=\u001b[39m (predictions[mask] \u001b[38;5;241m==\u001b[39m graph_data\u001b[38;5;241m.\u001b[39my[mask])\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     20\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(h)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Second layer\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    loss_value, train_acc = train_model()\n",
    "    val_acc = evaluate_model(graph_data.val_mask)  # Evaluate on validation set\n",
    "    \n",
    "    # Save loss and accuracy for each epoch in pre-allocated arrays\n",
    "    losses[epoch] = loss_value\n",
    "    train_accs[epoch] = train_acc\n",
    "    val_accs[epoch] = val_acc\n",
    "    \n",
    "    #if (epoch+1) % 10 == 0:\n",
    "    print(f'Epoch: {epoch+1:03d}, Loss: {loss_value:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# After training, evaluate the model on the test set\n",
    "test_acc = evaluate_model(graph_data.test_mask)  # Evaluate on test set\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
