{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country', 'metal', 'jazz', 'hiphop', 'blues', 'classical', 'reggae', 'rock', 'pop', 'disco']\n"
     ]
    }
   ],
   "source": [
    "# GTZAN dataset\n",
    "general_path = 'Data'\n",
    "print(list(os.listdir(f'{general_path}/genres_original/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Steps\n",
    "# Ensure all MEL-spectrograms are same NxN dimensions\n",
    "# Convert them into compressed vector representations\n",
    "# create train-test-split,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>length</th>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc16_var</th>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <th>mfcc17_var</th>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <th>mfcc18_var</th>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <th>mfcc19_var</th>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <th>mfcc20_var</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blues.00000.0.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.335406</td>\n",
       "      <td>0.091048</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>1773.065032</td>\n",
       "      <td>167541.630869</td>\n",
       "      <td>1972.744388</td>\n",
       "      <td>117335.771563</td>\n",
       "      <td>...</td>\n",
       "      <td>39.687145</td>\n",
       "      <td>-3.241280</td>\n",
       "      <td>36.488243</td>\n",
       "      <td>0.722209</td>\n",
       "      <td>38.099152</td>\n",
       "      <td>-5.050335</td>\n",
       "      <td>33.618073</td>\n",
       "      <td>-0.243027</td>\n",
       "      <td>43.771767</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blues.00000.1.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.343065</td>\n",
       "      <td>0.086147</td>\n",
       "      <td>0.112699</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>1816.693777</td>\n",
       "      <td>90525.690866</td>\n",
       "      <td>2010.051501</td>\n",
       "      <td>65671.875673</td>\n",
       "      <td>...</td>\n",
       "      <td>64.748276</td>\n",
       "      <td>-6.055294</td>\n",
       "      <td>40.677654</td>\n",
       "      <td>0.159015</td>\n",
       "      <td>51.264091</td>\n",
       "      <td>-2.837699</td>\n",
       "      <td>97.030830</td>\n",
       "      <td>5.784063</td>\n",
       "      <td>59.943081</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blues.00000.2.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.346815</td>\n",
       "      <td>0.092243</td>\n",
       "      <td>0.132003</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>1788.539719</td>\n",
       "      <td>111407.437613</td>\n",
       "      <td>2084.565132</td>\n",
       "      <td>75124.921716</td>\n",
       "      <td>...</td>\n",
       "      <td>67.336563</td>\n",
       "      <td>-1.768610</td>\n",
       "      <td>28.348579</td>\n",
       "      <td>2.378768</td>\n",
       "      <td>45.717648</td>\n",
       "      <td>-1.938424</td>\n",
       "      <td>53.050835</td>\n",
       "      <td>2.517375</td>\n",
       "      <td>33.105122</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blues.00000.3.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.363639</td>\n",
       "      <td>0.086856</td>\n",
       "      <td>0.132565</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>1655.289045</td>\n",
       "      <td>111952.284517</td>\n",
       "      <td>1960.039988</td>\n",
       "      <td>82913.639269</td>\n",
       "      <td>...</td>\n",
       "      <td>47.739452</td>\n",
       "      <td>-3.841155</td>\n",
       "      <td>28.337118</td>\n",
       "      <td>1.218588</td>\n",
       "      <td>34.770935</td>\n",
       "      <td>-3.580352</td>\n",
       "      <td>50.836224</td>\n",
       "      <td>3.630866</td>\n",
       "      <td>32.023678</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blues.00000.4.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.335579</td>\n",
       "      <td>0.088129</td>\n",
       "      <td>0.143289</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>1630.656199</td>\n",
       "      <td>79667.267654</td>\n",
       "      <td>1948.503884</td>\n",
       "      <td>60204.020268</td>\n",
       "      <td>...</td>\n",
       "      <td>30.336359</td>\n",
       "      <td>0.664582</td>\n",
       "      <td>45.880913</td>\n",
       "      <td>1.689446</td>\n",
       "      <td>51.363583</td>\n",
       "      <td>-3.392489</td>\n",
       "      <td>26.738789</td>\n",
       "      <td>0.536961</td>\n",
       "      <td>29.146694</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  length  chroma_stft_mean  chroma_stft_var  rms_mean  \\\n",
       "0  blues.00000.0.wav   66149          0.335406         0.091048  0.130405   \n",
       "1  blues.00000.1.wav   66149          0.343065         0.086147  0.112699   \n",
       "2  blues.00000.2.wav   66149          0.346815         0.092243  0.132003   \n",
       "3  blues.00000.3.wav   66149          0.363639         0.086856  0.132565   \n",
       "4  blues.00000.4.wav   66149          0.335579         0.088129  0.143289   \n",
       "\n",
       "    rms_var  spectral_centroid_mean  spectral_centroid_var  \\\n",
       "0  0.003521             1773.065032          167541.630869   \n",
       "1  0.001450             1816.693777           90525.690866   \n",
       "2  0.004620             1788.539719          111407.437613   \n",
       "3  0.002448             1655.289045          111952.284517   \n",
       "4  0.001701             1630.656199           79667.267654   \n",
       "\n",
       "   spectral_bandwidth_mean  spectral_bandwidth_var  ...  mfcc16_var  \\\n",
       "0              1972.744388           117335.771563  ...   39.687145   \n",
       "1              2010.051501            65671.875673  ...   64.748276   \n",
       "2              2084.565132            75124.921716  ...   67.336563   \n",
       "3              1960.039988            82913.639269  ...   47.739452   \n",
       "4              1948.503884            60204.020268  ...   30.336359   \n",
       "\n",
       "   mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  mfcc19_var  \\\n",
       "0    -3.241280   36.488243     0.722209   38.099152    -5.050335   33.618073   \n",
       "1    -6.055294   40.677654     0.159015   51.264091    -2.837699   97.030830   \n",
       "2    -1.768610   28.348579     2.378768   45.717648    -1.938424   53.050835   \n",
       "3    -3.841155   28.337118     1.218588   34.770935    -3.580352   50.836224   \n",
       "4     0.664582   45.880913     1.689446   51.363583    -3.392489   26.738789   \n",
       "\n",
       "   mfcc20_mean  mfcc20_var  label  \n",
       "0    -0.243027   43.771767  blues  \n",
       "1     5.784063   59.943081  blues  \n",
       "2     2.517375   33.105122  blues  \n",
       "3     3.630866   32.023678  blues  \n",
       "4     0.536961   29.146694  blues  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/features_3_sec.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9990, 60)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <th>rolloff_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc16_var</th>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <th>mfcc17_var</th>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <th>mfcc18_var</th>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <th>mfcc19_var</th>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <th>mfcc20_var</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66149</td>\n",
       "      <td>0.335406</td>\n",
       "      <td>0.091048</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>1773.065032</td>\n",
       "      <td>167541.630869</td>\n",
       "      <td>1972.744388</td>\n",
       "      <td>117335.771563</td>\n",
       "      <td>3714.560359</td>\n",
       "      <td>...</td>\n",
       "      <td>39.687145</td>\n",
       "      <td>-3.241280</td>\n",
       "      <td>36.488243</td>\n",
       "      <td>0.722209</td>\n",
       "      <td>38.099152</td>\n",
       "      <td>-5.050335</td>\n",
       "      <td>33.618073</td>\n",
       "      <td>-0.243027</td>\n",
       "      <td>43.771767</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66149</td>\n",
       "      <td>0.343065</td>\n",
       "      <td>0.086147</td>\n",
       "      <td>0.112699</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>1816.693777</td>\n",
       "      <td>90525.690866</td>\n",
       "      <td>2010.051501</td>\n",
       "      <td>65671.875673</td>\n",
       "      <td>3869.682242</td>\n",
       "      <td>...</td>\n",
       "      <td>64.748276</td>\n",
       "      <td>-6.055294</td>\n",
       "      <td>40.677654</td>\n",
       "      <td>0.159015</td>\n",
       "      <td>51.264091</td>\n",
       "      <td>-2.837699</td>\n",
       "      <td>97.030830</td>\n",
       "      <td>5.784063</td>\n",
       "      <td>59.943081</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66149</td>\n",
       "      <td>0.346815</td>\n",
       "      <td>0.092243</td>\n",
       "      <td>0.132003</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>1788.539719</td>\n",
       "      <td>111407.437613</td>\n",
       "      <td>2084.565132</td>\n",
       "      <td>75124.921716</td>\n",
       "      <td>3997.639160</td>\n",
       "      <td>...</td>\n",
       "      <td>67.336563</td>\n",
       "      <td>-1.768610</td>\n",
       "      <td>28.348579</td>\n",
       "      <td>2.378768</td>\n",
       "      <td>45.717648</td>\n",
       "      <td>-1.938424</td>\n",
       "      <td>53.050835</td>\n",
       "      <td>2.517375</td>\n",
       "      <td>33.105122</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66149</td>\n",
       "      <td>0.363639</td>\n",
       "      <td>0.086856</td>\n",
       "      <td>0.132565</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>1655.289045</td>\n",
       "      <td>111952.284517</td>\n",
       "      <td>1960.039988</td>\n",
       "      <td>82913.639269</td>\n",
       "      <td>3568.300218</td>\n",
       "      <td>...</td>\n",
       "      <td>47.739452</td>\n",
       "      <td>-3.841155</td>\n",
       "      <td>28.337118</td>\n",
       "      <td>1.218588</td>\n",
       "      <td>34.770935</td>\n",
       "      <td>-3.580352</td>\n",
       "      <td>50.836224</td>\n",
       "      <td>3.630866</td>\n",
       "      <td>32.023678</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66149</td>\n",
       "      <td>0.335579</td>\n",
       "      <td>0.088129</td>\n",
       "      <td>0.143289</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>1630.656199</td>\n",
       "      <td>79667.267654</td>\n",
       "      <td>1948.503884</td>\n",
       "      <td>60204.020268</td>\n",
       "      <td>3469.992864</td>\n",
       "      <td>...</td>\n",
       "      <td>30.336359</td>\n",
       "      <td>0.664582</td>\n",
       "      <td>45.880913</td>\n",
       "      <td>1.689446</td>\n",
       "      <td>51.363583</td>\n",
       "      <td>-3.392489</td>\n",
       "      <td>26.738789</td>\n",
       "      <td>0.536961</td>\n",
       "      <td>29.146694</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  chroma_stft_mean  chroma_stft_var  rms_mean   rms_var  \\\n",
       "0   66149          0.335406         0.091048  0.130405  0.003521   \n",
       "1   66149          0.343065         0.086147  0.112699  0.001450   \n",
       "2   66149          0.346815         0.092243  0.132003  0.004620   \n",
       "3   66149          0.363639         0.086856  0.132565  0.002448   \n",
       "4   66149          0.335579         0.088129  0.143289  0.001701   \n",
       "\n",
       "   spectral_centroid_mean  spectral_centroid_var  spectral_bandwidth_mean  \\\n",
       "0             1773.065032          167541.630869              1972.744388   \n",
       "1             1816.693777           90525.690866              2010.051501   \n",
       "2             1788.539719          111407.437613              2084.565132   \n",
       "3             1655.289045          111952.284517              1960.039988   \n",
       "4             1630.656199           79667.267654              1948.503884   \n",
       "\n",
       "   spectral_bandwidth_var  rolloff_mean  ...  mfcc16_var  mfcc17_mean  \\\n",
       "0           117335.771563   3714.560359  ...   39.687145    -3.241280   \n",
       "1            65671.875673   3869.682242  ...   64.748276    -6.055294   \n",
       "2            75124.921716   3997.639160  ...   67.336563    -1.768610   \n",
       "3            82913.639269   3568.300218  ...   47.739452    -3.841155   \n",
       "4            60204.020268   3469.992864  ...   30.336359     0.664582   \n",
       "\n",
       "   mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  mfcc19_var  mfcc20_mean  \\\n",
       "0   36.488243     0.722209   38.099152    -5.050335   33.618073    -0.243027   \n",
       "1   40.677654     0.159015   51.264091    -2.837699   97.030830     5.784063   \n",
       "2   28.348579     2.378768   45.717648    -1.938424   53.050835     2.517375   \n",
       "3   28.337118     1.218588   34.770935    -3.580352   50.836224     3.630866   \n",
       "4   45.880913     1.689446   51.363583    -3.392489   26.738789     0.536961   \n",
       "\n",
       "   mfcc20_var  label  \n",
       "0   43.771767  blues  \n",
       "1   59.943081  blues  \n",
       "2   33.105122  blues  \n",
       "3   32.023678  blues  \n",
       "4   29.146694  blues  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Some logic derived from reading through the following Kaggle set: https://www.kaggle.com/code/aishwarya2210/let-s-tune-the-music-with-cnn-xgboost/notebook\n",
    "This is for scaling and extracting the features as needed, and converting the labels to numerical values. \n",
    "This will help to create the training and testing sets, then will be fed into the model\n",
    "'''\n",
    "\n",
    "# Label Encoding - encod the categorical classes with numerical integer values for training\n",
    "\n",
    "# Blues - 0\n",
    "# Classical - 1\n",
    "# Country - 2\n",
    "# Disco - 3\n",
    "# Hip-hop - 4 \n",
    "# Jazz - 5  \n",
    "# Metal - 6 \n",
    "# Pop - 7\n",
    "# Reggae - 8\n",
    "# Rock - 9\n",
    "\n",
    "#To convert categorical data into model-understandable numerica data\n",
    "class_list = df.iloc[:, -1]\n",
    "convertor = LabelEncoder()\n",
    "\n",
    "df = df.drop(labels=\"filename\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the label encoder & return encoded labels\n",
    "y = convertor.fit_transform(class_list)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fit = StandardScaler()\n",
    "X = fit.fit_transform(np.array(df.iloc[:, :-1], dtype = float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6993, 58)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(df.iloc[:,:-1],y,test_size=0.3)\n",
    "x_train.head()\n",
    "# y_train.item(1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[9990, 57], edge_index=[2, 99790110], y=[9990], train_mask=[9990], val_mask=[9990], test_mask=[9990])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IGNORING PREVIOUSLY MADE TRAIN/TEST SPLIT\n",
    "\n",
    "# Removing other unecessary variables\n",
    "df = df.drop(labels=\"length\", axis=1)\n",
    "df = df.drop(labels=\"label\", axis=1)\n",
    "\n",
    "# Creataing edge tensor, 1 - Identity Matrix --> tensor\n",
    "adj_matrix = 1 - torch.eye(df.shape[0])\n",
    "edge_index = torch.nonzero(adj_matrix, as_tuple=False).t()\n",
    "\n",
    "# Setting data features to tensor\n",
    "x = torch.tensor(df.values)\n",
    "x = x.to(torch.float32)\n",
    "\n",
    "# NORMALIZE?\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# x = torch.tensor(scaler.fit_transform(df.values), dtype=torch.float32)\n",
    "\n",
    "# Setting y to tensor\n",
    "y = torch.tensor(y, dtype=torch.long)  # Make sure target is of type long (integer labels)\n",
    "\n",
    "# Create data object\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Setup test/train/validation split 80/10/10:\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split the data into 80% training and 20% remaining (test + validation)\n",
    "train_indices, remaining_indices = train_test_split(\n",
    "    range(df.shape[0]), test_size=0.2, stratify=y.numpy(), random_state=42\n",
    ")\n",
    "# split the remaining 20% into 50% for validation and 50% for testing\n",
    "val_indices, test_indices = train_test_split(\n",
    "    remaining_indices, test_size=0.5, stratify=y.numpy()[remaining_indices], random_state=42\n",
    ")\n",
    "# initialize masks for each split\n",
    "train_mask = torch.zeros(df.shape[0], dtype=torch.bool)\n",
    "val_mask = torch.zeros(df.shape[0], dtype=torch.bool)\n",
    "test_mask = torch.zeros(df.shape[0], dtype=torch.bool)\n",
    "# set the corresponding entries to True for each mask\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "# assign the masks to the data object\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Check dimmensions\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([9990, 57])\n",
      "edge_index shape: torch.Size([2, 99790110])\n",
      "y shape: torch.Size([9990])\n",
      "train_mask shape: torch.Size([9990])\n",
      "val_mask shape: torch.Size([9990])\n",
      "test_mask shape: torch.Size([9990])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x shape: {data.x.shape}\")\n",
    "print(f\"edge_index shape: {data.edge_index.shape}\")\n",
    "print(f\"y shape: {data.y.shape}\")\n",
    "print(f\"train_mask shape: {data.train_mask.shape}\")\n",
    "print(f\"val_mask shape: {data.val_mask.shape}\")\n",
    "print(f\"test_mask shape: {data.test_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://www.geeksforgeeks.org/graph-neural-networks-with-pytorch/\n",
    "\n",
    "### OLD OLD OLD ###\n",
    "### OLD OLD OLD ###\n",
    "### OLD OLD OLD ###\n",
    "\n",
    "class CustomGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(CustomGNN, self).__init__()\n",
    "        self.layer1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.layer2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, feature_data, edge_info):\n",
    "        # First GCN layer\n",
    "        x = self.layer1(feature_data, edge_info)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # Second GCN layer\n",
    "        x = self.layer2(x, edge_info)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize the GNN model\n",
    "input_features = df.shape[1]\n",
    "hidden_dim = df.shape[1] # PICK!\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "weight_decay=5e-4\n",
    "\n",
    "model = CustomGNN(input_features, hidden_dim, num_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "graph_data = data  # Get the graph data\n",
    "\n",
    "def train_model():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(graph_data.x, graph_data.edge_index)\n",
    "    loss = F.nll_loss(output[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss_value = train_model()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch+1:03d}, Loss: {loss_value:.4f}')\n",
    "\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data.x, graph_data.edge_index).argmax(dim=1)\n",
    "        correct = (predictions[graph_data.test_mask] == graph_data.y[graph_data.test_mask]).sum()\n",
    "        acc = int(correct) / int(graph_data.test_mask.sum())\n",
    "    return acc\n",
    "\n",
    "accuracy = evaluate_model()\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Modified from https://www.geeksforgeeks.org/graph-neural-networks-with-pytorch/\n",
    "\n",
    "class CustomGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(CustomGNN, self).__init__()\n",
    "        self.layer1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.layer2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, feature_data, edge_info):\n",
    "        # First GCN layer\n",
    "        x = self.layer1(feature_data, edge_info)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # Second GCN layer\n",
    "        x = self.layer2(x, edge_info)\n",
    "        return x  # Output raw logits, no log_softmax\n",
    "\n",
    "# Initialize the GNN model\n",
    "input_feature_count = df.shape[1]\n",
    "hidden_dim = df.shape[1]  # PICK!\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "weight_decay = 5e-4\n",
    "\n",
    "model = CustomGNN(input_feature_count, hidden_dim, num_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "graph_data = data  # Get the graph data\n",
    "\n",
    "# Cross-entropy loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(graph_data.x, graph_data.edge_index)\n",
    "    # Use CrossEntropyLoss directly, no need for log_softmax\n",
    "    loss = criterion(output[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss_value = train_model()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch+1:03d}, Loss: {loss_value:.4f}')\n",
    "\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data.x, graph_data.edge_index).argmax(dim=1)\n",
    "        correct = (predictions[graph_data.test_mask] == graph_data.y[graph_data.test_mask]).sum()\n",
    "        acc = int(correct) / int(graph_data.test_mask.sum())\n",
    "    return acc\n",
    "\n",
    "accuracy = evaluate_model()\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (layer1): GCNConv(57, 57)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (layer2): GCNConv(57, 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Modified from https://www.geeksforgeeks.org/graph-neural-networks-with-pytorch/\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layer1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.layer2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #1st Layer\n",
    "        h = self.layer1(x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        #Dropout\n",
    "        h = self.dropout(h)\n",
    "        #2nd Layer\n",
    "        z = self.layer2(h, edge_index)\n",
    "        return z\n",
    "    \n",
    "model_test = GCN(df.shape[1], df.shape[1], 10, 0.5)\n",
    "print(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GNN model\n",
    "input_feature_count = df.shape[1]\n",
    "hidden_dim = df.shape[1]  # PICK!\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "weight_decay = 5e-4\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = GCN(input_feature_count, hidden_dim, num_classes, dropout_rate)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "graph_data = data  # Get the graph data\n",
    "\n",
    "# Cross-entropy loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(graph_data.x, graph_data.edge_index)\n",
    "    loss = criterion(output[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Training accuracy\n",
    "    predictions = output.argmax(dim=1)\n",
    "    correct = (predictions[graph_data.train_mask] == graph_data.y[graph_data.train_mask]).sum()\n",
    "    train_acc = int(correct) / int(graph_data.train_mask.sum())\n",
    "    return loss.item(), train_acc\n",
    "\n",
    "def evaluate_model(mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(graph_data.x, graph_data.edge_index)\n",
    "        predictions = output.argmax(dim=1)\n",
    "        correct = (predictions[mask] == graph_data.y[mask]).sum()\n",
    "        acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    loss_value, train_acc = train_model()\n",
    "    val_acc = evaluate_model(graph_data.val_mask)  # Evaluate on validation set\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch: {epoch+1:03d}, Loss: {loss_value:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# After training, evaluate the model on the test set\n",
    "test_acc = evaluate_model(graph_data.test_mask)  # Evaluate on test set\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
